{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed62fca0",
   "metadata": {},
   "source": [
    "## Using Agents in LlamaIndex\n",
    "### Environement Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb463076",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab36bad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_config import MyConfig\n",
    "my_config = MyConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a45368",
   "metadata": {},
   "source": [
    "`LlamaIndex` supports three main types of reasoning agents:\n",
    "\n",
    "1. `Function Calling Agents` - These work with AI models that can call specific functions.\n",
    "2. `ReAct Agents` - These can work with any AI that does chat or text endpoint and deal with complex reasoning tasks.\n",
    "3. `Advanced Custom Agents` - These use more complex methods to deal with more complex tasks and workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6118b63b",
   "metadata": {},
   "source": [
    "### Basic Agents and States"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dedad4",
   "metadata": {},
   "source": [
    "#### Getting My LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5db9cb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/potter/Documents/MLArena/MLOps-Platform-Project/Hands-On/hfagnt-2-2-the-llamaindex-framework/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from my_utils import RunPodLLamaAgentQwenLLM\n",
    "model_id=\"Qwen/Qwen2.5-Coder-7B-Instruct\"\n",
    "vllm_api_base = f\"https://{my_config.VLLM_LLM_INFERENCE_NODE_IP}-8000.proxy.runpod.net/v1/chat/completions\"\n",
    "llm = RunPodLLamaAgentQwenLLM(api_url=vllm_api_base, overriden_model_name=model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd22344",
   "metadata": {},
   "source": [
    "### Very Basic Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9268fdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import AgentWorkflow, FunctionAgent\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "# define sample Tool -- type annotations, function names, and docstrings, are all included in parsed schemas!\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiplies two integers and returns the resulting integer\"\"\"\n",
    "    return a * b\n",
    "\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds two integers and returns the resulting integer\"\"\"\n",
    "    return a + b\n",
    "\n",
    "# initialize agent\n",
    "agent = AgentWorkflow.from_tools_or_functions(\n",
    "    [FunctionTool.from_defaults(multiply), FunctionTool.from_defaults(add)],\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# workflow = FunctionAgent(\n",
    "#     agent=[multiply, add],\n",
    "#     llm=llm,\n",
    "#     system_prompt=\"You are an agent that can perform basic mathematical operations using tools.\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0416b3d",
   "metadata": {},
   "source": [
    "**Agents are stateless by default**, add remembering past interactions is opt-in using a `Context` object This might be useful if you want to use an agent that needs to remember previous interactions, like a chatbot that maintains context across multiple messages or a task manager that needs to track progress over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be1f85ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response: assistant: Thought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: multiply\n",
      "Action Input: {\"a\": 2, \"b\": 4}\n",
      "Model response: assistant: Thought: Now I have the result of the multiplication. I need to add 20 to it.\n",
      "Action: add\n",
      "Action Input: {'a': 20, 'b': 8}\n",
      "Model response: assistant: Thought: I can answer without using any more tools. I'll use the user's language to answer.\n",
      "Answer: 28\n",
      "assistant: 28\n"
     ]
    }
   ],
   "source": [
    "# stateless\n",
    "# response = await agent.run(\"What is 2 times 2?\")\n",
    "# response\n",
    "response = await agent.run(user_msg=\"What is 20+(2*4)?\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ed498ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response: assistant: Thought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Answer: Hello, Bob! How can I assist you today?\n",
      "Model response: assistant: Thought: I can answer without using any more tools. I'll use the user's language to answer.\n",
      "Answer: Your name is Bob.\n"
     ]
    }
   ],
   "source": [
    "# remembering state\n",
    "from llama_index.core.workflow import Context\n",
    "\n",
    "ctx = Context(agent)\n",
    "\n",
    "response = await agent.run(\"My name is Bob.\", ctx=ctx)\n",
    "response = await agent.run(\"What was my name again?\", ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f044bd7a",
   "metadata": {},
   "source": [
    "### Very Basic Workflow with Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff08e29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import Context\n",
    "from llama_index.core.agent.workflow import ReActAgent, AgentWorkflow\n",
    "\n",
    "# Define some tools\n",
    "async def add(ctx: Context, a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers.\"\"\"\n",
    "    # update our count\n",
    "    cur_state = await ctx.get(\"state\")\n",
    "    cur_state[\"num_fn_calls\"] += 1\n",
    "    print(cur_state)\n",
    "    await ctx.set(\"state\", cur_state)\n",
    "    return a + b\n",
    "\n",
    "async def multiply(ctx: Context, a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    # update our count\n",
    "    cur_state = await ctx.get(\"state\")\n",
    "    cur_state[\"num_fn_calls\"] += 1\n",
    "    print(cur_state)\n",
    "    await ctx.set(\"state\", cur_state)\n",
    "    return a * b\n",
    "\n",
    "\n",
    "\n",
    "# we can pass functions directly without FunctionTool -- the fn/docstring are parsed for the name/description\n",
    "multiply_agent = ReActAgent(\n",
    "    name=\"multiply_agent\",\n",
    "    description=\"Is able to multiply two integers\",\n",
    "    system_prompt=\"A helpful assistant that can use a tool to multiply numbers.\",\n",
    "    tools=[multiply],\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "addition_agent = ReActAgent(\n",
    "    name=\"add_agent\",\n",
    "    description=\"Is able to add two integers\",\n",
    "    system_prompt=\"A helpful assistant that can use a tool to add numbers.\",\n",
    "    tools=[add],\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "workflow = AgentWorkflow(\n",
    "    agents=[multiply_agent, addition_agent],\n",
    "    root_agent=\"multiply_agent\",\n",
    "    initial_state={\"num_fn_calls\": 0},\n",
    "    state_prompt=\"Current state: {state}. User message: {msg}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cd8058f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response: assistant: Thought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: multiply\n",
      "Action Input: {\"a\": 3, \"b\": 5}\n",
      "{'num_fn_calls': 5}\n",
      "Model response: assistant: Thought: I need to use another multiplication operation before adding the result.\n",
      "Action: multiply\n",
      "Action Input: {'a': 10, 'b': 13}\n",
      "{'num_fn_calls': 6}\n",
      "Model response: assistant: Thought: Now I need to multiply the result by 10 and then add it to the previous result.\n",
      "Action: multiply\n",
      "Action Input: {'a': 130, 'b': 10}\n",
      "{'num_fn_calls': 7}\n",
      "Model response: assistant: Thought: Finally, I need to add the results of the first multiplication to the result of the last multiplication.\n",
      "Action: multiply\n",
      "Action Input: {'a': 15, 'b': 1300}\n",
      "{'num_fn_calls': 8}\n",
      "Model response: assistant: Thought: I can answer without using any more tools. I'll use the user's language to answer.\n",
      "Answer: The output of (3*5)+(10*13)*10 is 19500.\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# run the workflow with context\n",
    "ctx = Context(workflow)\n",
    "response = await workflow.run(user_msg=\"What is the output of (3*5)+(10*13)*10?\", ctx=ctx)\n",
    "\n",
    "# pull out and inspect the state\n",
    "state = await ctx.get(\"state\")\n",
    "print(state[\"num_fn_calls\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
