{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed62fca0",
   "metadata": {},
   "source": [
    "## Using Agents in LlamaIndex\n",
    "### Environement Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb463076",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab36bad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_config import MyConfig\n",
    "my_config = MyConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a45368",
   "metadata": {},
   "source": [
    "`LlamaIndex` supports three main types of reasoning agents:\n",
    "\n",
    "1. `Function Calling Agents` - These work with AI models that can call specific functions.\n",
    "2. `ReAct Agents` - These can work with any AI that does chat or text endpoint and deal with complex reasoning tasks.\n",
    "3. `Advanced Custom Agents` - These use more complex methods to deal with more complex tasks and workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6118b63b",
   "metadata": {},
   "source": [
    "### Basic Agents and States"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dedad4",
   "metadata": {},
   "source": [
    "#### Getting My LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5db9cb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/potter/Documents/MLArena/MLOps-Platform-Project/Hands-On/hfagnt-2-2-the-llamaindex-framework/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from my_utils import RunPodLLamaAgentQwenLLM\n",
    "model_id=\"Qwen/Qwen2.5-Coder-7B-Instruct\"\n",
    "vllm_api_base = f\"https://{my_config.VLLM_LLM_INFERENCE_NODE_IP}-8000.proxy.runpod.net/v1/chat/completions\"\n",
    "llm = RunPodLLamaAgentQwenLLM(api_url=vllm_api_base, overriden_model_name=model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9268fdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import AgentWorkflow, FunctionAgent\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "# define sample Tool -- type annotations, function names, and docstrings, are all included in parsed schemas!\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiplies two integers and returns the resulting integer\"\"\"\n",
    "    return a * b\n",
    "\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds two integers and returns the resulting integer\"\"\"\n",
    "    return a + b\n",
    "\n",
    "# initialize agent\n",
    "agent = AgentWorkflow.from_tools_or_functions(\n",
    "    [FunctionTool.from_defaults(multiply), FunctionTool.from_defaults(add)],\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# workflow = FunctionAgent(\n",
    "#     agent=[multiply, add],\n",
    "#     llm=llm,\n",
    "#     system_prompt=\"You are an agent that can perform basic mathematical operations using tools.\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0416b3d",
   "metadata": {},
   "source": [
    "**Agents are stateless by default**, add remembering past interactions is opt-in using a `Context` object This might be useful if you want to use an agent that needs to remember previous interactions, like a chatbot that maintains context across multiple messages or a task manager that needs to track progress over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be1f85ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Called with args: kwargs: {}\n",
      "Model response: assistant: Thought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: multiply\n",
      "Action Input: {\"a\": 2, \"b\": 2}\n",
      "Called with args: kwargs: {}\n",
      "Model response: assistant: Thought: I can answer without using any more tools. I'll use the user's language to answer\n",
      "Answer: 2 times 2 is 4.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AgentOutput(response=ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='2 times 2 is 4.')]), tool_calls=[ToolCallResult(tool_name='multiply', tool_kwargs={'a': 2, 'b': 2}, tool_id='a3408c1a-4699-4f01-bbc1-faa872c80d25', tool_output=ToolOutput(content='4', tool_name='multiply', raw_input={'args': (), 'kwargs': {'a': 2, 'b': 2}}, raw_output=4, is_error=False), return_direct=False)], raw=None, current_agent_name='Agent')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stateless\n",
    "response = await agent.run(\"What is 2 times 2?\")\n",
    "response\n",
    "# response = await workflow.run(user_msg=\"What is 20+(2*4)?\")\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ed498ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Called with args: kwargs: {}\n",
      "Model response: assistant: Thought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Answer: Hello, Bob! How can I assist you today?\n",
      "Called with args: kwargs: {}\n",
      "Model response: assistant: Thought: I can answer without using any more tools. I'll use the user's language to answer.\n",
      "Answer: Your name was Bob.\n"
     ]
    }
   ],
   "source": [
    "# remembering state\n",
    "from llama_index.core.workflow import Context\n",
    "\n",
    "ctx = Context(agent)\n",
    "\n",
    "response = await agent.run(\"My name is Bob.\", ctx=ctx)\n",
    "response = await agent.run(\"What was my name again?\", ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f044bd7a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff08e29a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
