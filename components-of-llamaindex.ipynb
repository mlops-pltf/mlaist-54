{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "708b5568",
   "metadata": {},
   "source": [
    "## Create a `QueryEngine` for retrieval augmented generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd5f3b6",
   "metadata": {},
   "source": [
    "### Set up the environemnt first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "686fb7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98b2b11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_config import MyConfig\n",
    "my_config = MyConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9100bc",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "### Setting up the persona database\n",
    "We will be using personas from the dvilasuero/finepersonas-v0.1-tiny dataset. This dataset contains 5K personas that will be attending the party!\n",
    "Let's load the dataset and store it as files in the data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc799279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# from pathlib import Path\n",
    "\n",
    "# dataset = load_dataset(path=\"dvilasuero/finepersonas-v0.1-tiny\", split=\"train\")\n",
    "\n",
    "# Path(\"data\").mkdir(parents=True, exist_ok=True)\n",
    "# for i, persona in enumerate(dataset):\n",
    "#     with open(Path(\"data\") / f\"persona_{i}.txt\", \"w\") as f:\n",
    "#         f.write(persona[\"persona\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e818312",
   "metadata": {},
   "source": [
    "### Loading and embedding persona documents\n",
    "We will use the `SimpleDirectoryReader` to load the persona descriptions from the `data` directory. This will return a list of `Document` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddf64071",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/potter/Documents/MLArena/MLOps-Platform-Project/Hands-On/hfagnt-2-2-the-llamaindex-framework/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "reader = SimpleDirectoryReader(input_dir=\"data\")\n",
    "documents = reader.load_data()\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c87b79",
   "metadata": {},
   "source": [
    "Now we have a list of `Document` objects, we can use the `IngestionPipeline` to create nodes from the documents and prepare them for the `QueryEngine`.\n",
    "\n",
    "We will use the `SentenceSplitter` to split the documents into smaller chunks and the `HuggingFaceEmbedding` (via `vLLM` possibly) to embed the chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc65c85",
   "metadata": {},
   "source": [
    "#### Create a Custom Embedding Class\n",
    "We are subclassing `BaseEmbedding` from `llama_index` and override the `aget_text_embedding` method to hit my `RunPod` endpoint. Super Cool stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d673b6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.embeddings import BaseEmbedding\n",
    "import aiohttp\n",
    "from typing import Optional\n",
    "\n",
    "class RunPodEmbedding(BaseEmbedding):\n",
    "    endpoint_url: str\n",
    "\n",
    "    def __init__(self, endpoint_url: str, **kwargs):\n",
    "        super().__init__(endpoint_url=endpoint_url, **kwargs)\n",
    "\n",
    "    async def _aget_text_embedding(self, text: str):\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            payload = {\"input\": text}\n",
    "            async with session.post(self.endpoint_url, json=payload) as resp:\n",
    "                result = await resp.json()\n",
    "                return result[\"data\"][0][\"embedding\"]\n",
    "\n",
    "    def _get_text_embedding(self, text: str):\n",
    "        raise NotImplementedError(\"Sync embedding not implemented.\")\n",
    "\n",
    "    async def _aget_query_embedding(self, query: str):\n",
    "        return await self._aget_text_embedding(query)\n",
    "\n",
    "    def _get_query_embedding(self, query: str):\n",
    "        raise NotImplementedError(\"Sync embedding not implemented.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26888b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 1 of 26\n",
      "Processed batch 2 of 26\n",
      "Processed batch 3 of 26\n",
      "Processed batch 4 of 26\n",
      "Processed batch 5 of 26\n",
      "Processed batch 6 of 26\n",
      "Processed batch 7 of 26\n",
      "Processed batch 8 of 26\n",
      "Processed batch 9 of 26\n",
      "Processed batch 10 of 26\n",
      "Processed batch 11 of 26\n",
      "Processed batch 12 of 26\n",
      "Processed batch 13 of 26\n",
      "Processed batch 14 of 26\n",
      "Processed batch 15 of 26\n"
     ]
    },
    {
     "ename": "ClientConnectorDNSError",
     "evalue": "Cannot connect to host v6irazi2v8xomb-8000.proxy.runpod.net:443 ssl:default [nodename nor servname provided, or not known]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/MLArena/MLOps-Platform-Project/Hands-On/hfagnt-2-2-the-llamaindex-framework/.venv/lib/python3.9/site-packages/aiohttp/connector.py:1512\u001b[0m, in \u001b[0;36mTCPConnector._create_direct_connection\u001b[0;34m(self, req, traces, timeout, client_error)\u001b[0m\n\u001b[1;32m   1508\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;66;03m# Cancelling this lookup should not cancel the underlying lookup\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m     \u001b[38;5;66;03m#  or else the cancel event will get broadcast to all the waiters\u001b[39;00m\n\u001b[1;32m   1511\u001b[0m     \u001b[38;5;66;03m#  across all connections.\u001b[39;00m\n\u001b[0;32m-> 1512\u001b[0m     hosts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resolve_host(host, port, traces\u001b[38;5;241m=\u001b[39mtraces)\n\u001b[1;32m   1513\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/Documents/MLArena/MLOps-Platform-Project/Hands-On/hfagnt-2-2-the-llamaindex-framework/.venv/lib/python3.9/site-packages/aiohttp/connector.py:1128\u001b[0m, in \u001b[0;36mTCPConnector._resolve_host\u001b[0;34m(self, host, port, traces)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mshield(resolved_host_task)\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mCancelledError:\n",
      "File \u001b[0;32m~/Documents/MLArena/MLOps-Platform-Project/Hands-On/hfagnt-2-2-the-llamaindex-framework/.venv/lib/python3.9/site-packages/aiohttp/connector.py:1159\u001b[0m, in \u001b[0;36mTCPConnector._resolve_host_with_throttle\u001b[0;34m(self, key, host, port, futures, traces)\u001b[0m\n\u001b[1;32m   1157\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m trace\u001b[38;5;241m.\u001b[39msend_dns_resolvehost_start(host)\n\u001b[0;32m-> 1159\u001b[0m addrs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resolver\u001b[38;5;241m.\u001b[39mresolve(host, port, family\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_family)\n\u001b[1;32m   1160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m traces:\n",
      "File \u001b[0;32m~/Documents/MLArena/MLOps-Platform-Project/Hands-On/hfagnt-2-2-the-llamaindex-framework/.venv/lib/python3.9/site-packages/aiohttp/resolver.py:40\u001b[0m, in \u001b[0;36mThreadedResolver.resolve\u001b[0;34m(self, host, port, family)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mresolve\u001b[39m(\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m, host: \u001b[38;5;28mstr\u001b[39m, port: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, family: socket\u001b[38;5;241m.\u001b[39mAddressFamily \u001b[38;5;241m=\u001b[39m socket\u001b[38;5;241m.\u001b[39mAF_INET\n\u001b[1;32m     39\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[ResolveResult]:\n\u001b[0;32m---> 40\u001b[0m     infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop\u001b[38;5;241m.\u001b[39mgetaddrinfo(\n\u001b[1;32m     41\u001b[0m         host,\n\u001b[1;32m     42\u001b[0m         port,\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39msocket\u001b[38;5;241m.\u001b[39mSOCK_STREAM,\n\u001b[1;32m     44\u001b[0m         family\u001b[38;5;241m=\u001b[39mfamily,\n\u001b[1;32m     45\u001b[0m         flags\u001b[38;5;241m=\u001b[39m_AI_ADDRCONFIG,\n\u001b[1;32m     46\u001b[0m     )\n\u001b[1;32m     48\u001b[0m     hosts: List[ResolveResult] \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py:856\u001b[0m, in \u001b[0;36mBaseEventLoop.getaddrinfo\u001b[0;34m(self, host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    854\u001b[0m     getaddr_func \u001b[38;5;241m=\u001b[39m socket\u001b[38;5;241m.\u001b[39mgetaddrinfo\n\u001b[0;32m--> 856\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_in_executor(\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m, getaddr_func, host, port, family, \u001b[38;5;28mtype\u001b[39m, proto, flags)\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/concurrent/futures/thread.py:52\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/socket.py:953\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    952\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 953\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    954\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "\u001b[0;31mgaierror\u001b[0m: [Errno 8] nodename nor servname provided, or not known",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mClientConnectorDNSError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_number \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(documents), \u001b[38;5;241m200\u001b[39m):\n\u001b[1;32m     17\u001b[0m     batch \u001b[38;5;241m=\u001b[39m documents[batch_number:batch_number \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m200\u001b[39m]\n\u001b[0;32m---> 18\u001b[0m     nodes\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;01mawait\u001b[39;00m pipeline\u001b[38;5;241m.\u001b[39marun(documents\u001b[38;5;241m=\u001b[39mbatch))\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessed batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_number\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m200\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(documents)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m200\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m; time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# To avoid rate limiting\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MLArena/MLOps-Platform-Project/Hands-On/hfagnt-2-2-the-llamaindex-framework/.venv/lib/python3.9/site-packages/llama_index/core/instrumentation/dispatcher.py:369\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.async_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    362\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_,\n\u001b[1;32m    363\u001b[0m     bound_args\u001b[38;5;241m=\u001b[39mbound_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    366\u001b[0m     tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[1;32m    367\u001b[0m )\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 369\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m~/Documents/MLArena/MLOps-Platform-Project/Hands-On/hfagnt-2-2-the-llamaindex-framework/.venv/lib/python3.9/site-packages/llama_index/core/ingestion/pipeline.py:737\u001b[0m, in \u001b[0;36mIngestionPipeline.arun\u001b[0;34m(self, show_progress, documents, nodes, cache_collection, in_place, store_doc_text, num_workers, **kwargs)\u001b[0m\n\u001b[1;32m    735\u001b[0m         nodes: Sequence[BaseNode] \u001b[38;5;241m=\u001b[39m reduce(\u001b[38;5;28;01mlambda\u001b[39;00m x, y: x \u001b[38;5;241m+\u001b[39m y, result, [])  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    736\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 737\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m arun_transformations(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    738\u001b[0m         nodes_to_run,\n\u001b[1;32m    739\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformations,\n\u001b[1;32m    740\u001b[0m         show_progress\u001b[38;5;241m=\u001b[39mshow_progress,\n\u001b[1;32m    741\u001b[0m         cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisable_cache \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    742\u001b[0m         cache_collection\u001b[38;5;241m=\u001b[39mcache_collection,\n\u001b[1;32m    743\u001b[0m         in_place\u001b[38;5;241m=\u001b[39min_place,\n\u001b[1;32m    744\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    745\u001b[0m     )\n\u001b[1;32m    746\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m nodes\n\u001b[1;32m    748\u001b[0m nodes \u001b[38;5;241m=\u001b[39m nodes \u001b[38;5;129;01mor\u001b[39;00m []\n",
      "File \u001b[0;32m~/Documents/MLArena/MLOps-Platform-Project/Hands-On/hfagnt-2-2-the-llamaindex-framework/.venv/lib/python3.9/site-packages/llama_index/core/ingestion/pipeline.py:136\u001b[0m, in \u001b[0;36marun_transformations\u001b[0;34m(nodes, transformations, in_place, cache, cache_collection, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m         nodes \u001b[38;5;241m=\u001b[39m cached_nodes\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m         nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m transform\u001b[38;5;241m.\u001b[39macall(nodes, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    137\u001b[0m         cache\u001b[38;5;241m.\u001b[39mput(\u001b[38;5;28mhash\u001b[39m, nodes, collection\u001b[38;5;241m=\u001b[39mcache_collection)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/MLArena/MLOps-Platform-Project/Hands-On/hfagnt-2-2-the-llamaindex-framework/.venv/lib/python3.9/site-packages/llama_index/core/base/embeddings/base.py:573\u001b[0m, in \u001b[0;36mBaseEmbedding.acall\u001b[0;34m(self, nodes, **kwargs)\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21macall\u001b[39m(\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28mself\u001b[39m, nodes: Sequence[BaseNode], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[1;32m    572\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Sequence[BaseNode]:\n\u001b[0;32m--> 573\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maget_text_embedding_batch(\n\u001b[1;32m    574\u001b[0m         [node\u001b[38;5;241m.\u001b[39mget_content(metadata_mode\u001b[38;5;241m=\u001b[39mMetadataMode\u001b[38;5;241m.\u001b[39mEMBED) \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes],\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    576\u001b[0m     )\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m node, embedding \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(nodes, embeddings):\n\u001b[1;32m    579\u001b[0m         node\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m embedding\n",
      "File \u001b[0;32m~/Documents/MLArena/MLOps-Platform-Project/Hands-On/hfagnt-2-2-the-llamaindex-framework/.venv/lib/python3.9/site-packages/llama_index/core/instrumentation/dispatcher.py:369\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.async_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    362\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_,\n\u001b[1;32m    363\u001b[0m     bound_args\u001b[38;5;241m=\u001b[39mbound_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    366\u001b[0m     tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[1;32m    367\u001b[0m )\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 369\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m~/Documents/MLArena/MLOps-Platform-Project/Hands-On/hfagnt-2-2-the-llamaindex-framework/.venv/lib/python3.9/site-packages/llama_index/core/base/embeddings/base.py:513\u001b[0m, in \u001b[0;36mBaseEmbedding.aget_text_embedding_batch\u001b[0;34m(self, texts, show_progress)\u001b[0m\n\u001b[1;32m    511\u001b[0m                 nested_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39membeddings_coroutines)\n\u001b[1;32m    512\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 513\u001b[0m             nested_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39membeddings_coroutines)\n\u001b[1;32m    515\u001b[0m result_embeddings \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    516\u001b[0m     embedding \u001b[38;5;28;01mfor\u001b[39;00m embeddings \u001b[38;5;129;01min\u001b[39;00m nested_embeddings \u001b[38;5;28;01mfor\u001b[39;00m embedding \u001b[38;5;129;01min\u001b[39;00m embeddings\n\u001b[1;32m    517\u001b[0m ]\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/MLArena/MLOps-Platform-Project/Hands-On/hfagnt-2-2-the-llamaindex-framework/.venv/lib/python3.9/site-packages/llama_index/core/base/embeddings/base.py:276\u001b[0m, in \u001b[0;36mBaseEmbedding._aget_text_embeddings\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_aget_text_embeddings\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Embedding]:\n\u001b[1;32m    271\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;124;03m    Embed the input sequence of text asynchronously.\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03m    Subclasses can implement this method if batch queries are supported.\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;241m*\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aget_text_embedding(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[1;32m    278\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[14], line 14\u001b[0m, in \u001b[0;36mRunPodEmbedding._aget_text_embedding\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m aiohttp\u001b[38;5;241m.\u001b[39mClientSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m     13\u001b[0m     payload \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: text}\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m session\u001b[38;5;241m.\u001b[39mpost(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoint_url, json\u001b[38;5;241m=\u001b[39mpayload) \u001b[38;5;28;01mas\u001b[39;00m resp:\n\u001b[1;32m     15\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/MLArena/MLOps-Platform-Project/Hands-On/hfagnt-2-2-the-llamaindex-framework/.venv/lib/python3.9/site-packages/aiohttp/client.py:1480\u001b[0m, in \u001b[0;36m_BaseRequestContextManager.__aenter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1479\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__aenter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _RetType:\n\u001b[0;32m-> 1480\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resp: _RetType \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coro\n\u001b[1;32m   1481\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resp\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__aenter__\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/MLArena/MLOps-Platform-Project/Hands-On/hfagnt-2-2-the-llamaindex-framework/.venv/lib/python3.9/site-packages/aiohttp/client.py:768\u001b[0m, in \u001b[0;36mClientSession._request\u001b[0;34m(self, method, str_or_url, params, data, json, cookies, headers, skip_auto_headers, auth, allow_redirects, max_redirects, compress, chunked, expect100, raise_for_status, read_until_eof, proxy, proxy_auth, timeout, verify_ssl, fingerprint, ssl_context, ssl, server_hostname, proxy_headers, trace_request_ctx, read_bufsize, auto_decompress, max_line_size, max_field_size, middlewares)\u001b[0m\n\u001b[1;32m    765\u001b[0m     handler \u001b[38;5;241m=\u001b[39m _connect_and_send_request\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m handler(req)\n\u001b[1;32m    769\u001b[0m \u001b[38;5;66;03m# Client connector errors should not be retried\u001b[39;00m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[1;32m    771\u001b[0m     ConnectionTimeoutError,\n\u001b[1;32m    772\u001b[0m     ClientConnectorError,\n\u001b[1;32m    773\u001b[0m     ClientConnectorCertificateError,\n\u001b[1;32m    774\u001b[0m     ClientConnectorSSLError,\n\u001b[1;32m    775\u001b[0m ):\n",
      "File \u001b[0;32m~/Documents/MLArena/MLOps-Platform-Project/Hands-On/hfagnt-2-2-the-llamaindex-framework/.venv/lib/python3.9/site-packages/aiohttp/client.py:723\u001b[0m, in \u001b[0;36mClientSession._request.<locals>._connect_and_send_request\u001b[0;34m(req)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connector \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 723\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connector\u001b[38;5;241m.\u001b[39mconnect(\n\u001b[1;32m    724\u001b[0m         req, traces\u001b[38;5;241m=\u001b[39mtraces, timeout\u001b[38;5;241m=\u001b[39mreal_timeout\n\u001b[1;32m    725\u001b[0m     )\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mTimeoutError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    727\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ConnectionTimeoutError(\n\u001b[1;32m    728\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection timeout to host \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreq\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    729\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MLArena/MLOps-Platform-Project/Hands-On/hfagnt-2-2-the-llamaindex-framework/.venv/lib/python3.9/site-packages/aiohttp/connector.py:622\u001b[0m, in \u001b[0;36mBaseConnector.connect\u001b[0;34m(self, req, traces, timeout)\u001b[0m\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m trace \u001b[38;5;129;01min\u001b[39;00m traces:\n\u001b[1;32m    621\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m trace\u001b[38;5;241m.\u001b[39msend_connection_create_start()\n\u001b[0;32m--> 622\u001b[0m proto \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection(req, traces, timeout)\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m traces:\n\u001b[1;32m    624\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m trace \u001b[38;5;129;01min\u001b[39;00m traces:\n",
      "File \u001b[0;32m~/Documents/MLArena/MLOps-Platform-Project/Hands-On/hfagnt-2-2-the-llamaindex-framework/.venv/lib/python3.9/site-packages/aiohttp/connector.py:1189\u001b[0m, in \u001b[0;36mTCPConnector._create_connection\u001b[0;34m(self, req, traces, timeout)\u001b[0m\n\u001b[1;32m   1187\u001b[0m     _, proto \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_proxy_connection(req, traces, timeout)\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1189\u001b[0m     _, proto \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_direct_connection(req, traces, timeout)\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m proto\n",
      "File \u001b[0;32m~/Documents/MLArena/MLOps-Platform-Project/Hands-On/hfagnt-2-2-the-llamaindex-framework/.venv/lib/python3.9/site-packages/aiohttp/connector.py:1518\u001b[0m, in \u001b[0;36mTCPConnector._create_direct_connection\u001b[0;34m(self, req, traces, timeout, client_error)\u001b[0m\n\u001b[1;32m   1515\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;66;03m# in case of proxy it is not ClientProxyConnectionError\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m     \u001b[38;5;66;03m# it is problem of resolving proxy ip itself\u001b[39;00m\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ClientConnectorDNSError(req\u001b[38;5;241m.\u001b[39mconnection_key, exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m   1520\u001b[0m last_exc: Optional[\u001b[38;5;167;01mException\u001b[39;00m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1521\u001b[0m addr_infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_hosts_to_addr_infos(hosts)\n",
      "\u001b[0;31mClientConnectorDNSError\u001b[0m: Cannot connect to host v6irazi2v8xomb-8000.proxy.runpod.net:443 ssl:default [nodename nor servname provided, or not known]"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "# Instantiate custom class with your RunPod URL\n",
    "runpod_url = f\"https://{my_config.VLLM_INFERENCE_RUNPOD_ID}-8000.proxy.runpod.net/v1/embeddings\"\n",
    "embedding_model = RunPodEmbedding(endpoint_url=runpod_url)\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(),\n",
    "        embedding_model,\n",
    "    ]\n",
    ")\n",
    "\n",
    "nodes = list()\n",
    "for batch_number in range(0, len(documents), 200):\n",
    "    batch = documents[batch_number:batch_number + 200]\n",
    "    nodes.extend(await pipeline.arun(documents=batch))\n",
    "    print(f\"Processed batch {batch_number // 200 + 1} => {batch_number}:{batch_number + 200}\")\n",
    "    import time; time.sleep(2)  # To avoid rate limiting\n",
    "len(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e12ea1",
   "metadata": {},
   "source": [
    "### Storing and indexing documents\n",
    "Since we are using an ingestion pipeline, we can directly attach a vector store to the pipeline to populate it. In this case, we will use `Chroma` to store our documents. Let's run the pipeline again with the vector store attached. The `IngestionPipeline` caches the operations so this should be fast!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab5352b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "db = chromadb.PersistentClient(path=\"./alfred_chroma_db\")\n",
    "chroma_collection = db.get_or_create_collection(name=\"alfred\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "\n",
    "# Instantiate custom class with your RunPod URL\n",
    "runpod_url = \"https://h8zdxcaagexdrc-8000.proxy.runpod.net/v1/embeddings\"\n",
    "embedding_model = RunPodEmbedding(endpoint_url=runpod_url)\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(),\n",
    "        embedding_model\n",
    "    ],\n",
    "    vector_store=vector_store,\n",
    ")\n",
    "\n",
    "nodes = list()\n",
    "for batch in range(0, len(documents), 200):\n",
    "    nodes.extend(await pipeline.arun(documents=documents[batch:batch+200]))\n",
    "    print(f\"Processed batch {batch // 200 + 1} => {batch}:{batch + 200}\")\n",
    "len(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a074900",
   "metadata": {},
   "source": [
    "We can create a `VectorStoreIndex` from the vector store and use it to query the documents by passing the vector store and embedding model to the `from_vector_store()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "237cde77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "\n",
    "# Instantiate custom class with your RunPod URL\n",
    "runpod_url = \"https://h8zdxcaagexdrc-8000.proxy.runpod.net/v1/embeddings\"\n",
    "embedding_model = RunPodEmbedding(endpoint_url=runpod_url)\n",
    "# embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store, embed_model=embedding_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485a8d33",
   "metadata": {},
   "source": [
    "We don't need to worry about persisting the index to disk, as it is automatically saved within the `ChromaVectorStore` object and the passed directory path.\n",
    "\n",
    "### Querying the index\n",
    "Now that we have our index, we can use it to query the documents. Let's create a `QueryEngine` from the index and use it to query the documents using a specific response mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "42a30809",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "import aiohttp\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "449aba9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms.llm import LLM\n",
    "from llama_index.core.llms import ChatMessage, LLMMetadata, CompletionResponse  # Sometimes required\n",
    "import aiohttp\n",
    "from typing import List, Any\n",
    "\n",
    "class RunPodQwenLLM(LLM):\n",
    "    api_url: str  # Pydantic field\n",
    "\n",
    "    def __init__(self, api_url: str, **kwargs):\n",
    "        super().__init__(api_url=api_url, **kwargs)\n",
    "\n",
    "    # ðŸŸ¢ Async chat: core method for LlamaIndex RAG\n",
    "    async def achat(\n",
    "        self, messages: List[ChatMessage], **kwargs\n",
    "    ) -> str:\n",
    "        # Prepare payload\n",
    "        payload = {\n",
    "            \"model\": \"Qwen/Qwen2.5-Coder-7B-Instruct\",\n",
    "            \"messages\": [\n",
    "                {\"role\": m.role, \"content\": m.content} for m in messages\n",
    "            ],\n",
    "            \"temperature\": 0.7,\n",
    "        }\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            async with session.post(self.api_url, json=payload) as resp:\n",
    "                result = await resp.json()\n",
    "                output_text = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "                return ChatMessage(role=\"assistant\", content=output_text)\n",
    "\n",
    "    # ðŸ”´ All other required methods: stub/not implemented\n",
    "    async def astream_chat(self, messages: List[ChatMessage], **kwargs) -> Any:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    async def astream_complete(self, prompt: str, **kwargs) -> Any:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    async def chat(self, messages: List[ChatMessage], **kwargs) -> str:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def complete(self, prompt: str, **kwargs) -> str:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def stream_chat(self, messages: List[ChatMessage], **kwargs) -> Any:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def stream_complete(self, prompt: str, **kwargs) -> Any:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    async def acomplete(self, prompt: str, **kwargs) -> CompletionResponse:\n",
    "        payload = {\n",
    "            \"model\": \"Qwen/Qwen2.5-Coder-7B-Instruct\",   # Adjust model name as needed\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            \"max_tokens\": 1024,\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.95,\n",
    "        }\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            async with session.post(self.api_url, json=payload) as resp:\n",
    "                result = await resp.json()\n",
    "                output_text = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "                return CompletionResponse(\n",
    "                    text=output_text,\n",
    "                    usage=result.get(\"usage\", {}),\n",
    "                    model=self.metadata.model_name\n",
    "                )\n",
    "    \n",
    "    async def _get_query_embedding(self, query: str):\n",
    "        # Simple solution: use asyncio to run async in sync context (not efficient, but unblocks you)\n",
    "        import asyncio\n",
    "        return asyncio.run(self._aget_query_embedding(query))\n",
    "\n",
    "    @property\n",
    "    def metadata(self) -> LLMMetadata:\n",
    "        return LLMMetadata(\n",
    "            name=\"RunPodQwenLLM\",\n",
    "            description=\"RunPod Qwen LLM for chat completions\",\n",
    "            model_name=\"Qwen/Qwen2.5-Coder-7B-Instruct\",  # Adjust model name as needed\n",
    "            max_input_size=4096,  # Adjust based on the model's capabilities\n",
    "            max_output_tokens=1024,  # Adjust based on your needs\n",
    "            context_window=2048,  # Adjust based on your needs\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e00a8a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vllm_api_base = \"https://e9q6qi5px4g2md-8000.proxy.runpod.net/v1/chat/completions\"\n",
    "llm = RunPodQwenLLM(api_url=vllm_api_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "94d549bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    llm=llm,\n",
    "    response_mode=\"tree_summarize\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a230a420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response=\"**Persona:** Dr. Elena Vassiliou, an accomplished anthropologist specializing in Cypriot culture, history, and society. With over two decades of dedicated research and firsthand experience living in Cyprus, Dr. Vassiliou has become an authority on the intricate tapestry of Cypriot life. Her extensive fieldwork has provided her with invaluable insights into the daily customs, traditions, and social dynamics of the island's diverse communities. As a resident of Nicosia for ten years, she has witnessed firsthand how historical events have shaped contemporary Cypriot society and how traditional practices coexist with modern influences. Through her extensive travels across the Mediterranean, Dr. Vassiliou has also gained a broader perspective on regional cultures, enriching her understanding of Cyprus within a wider context. Her work continues to inspire and educate those interested in exploring the rich heritage and evolving identity of Cyprus.\", source_nodes=[NodeWithScore(node=TextNode(id_='8cbfc317-72ff-4cae-a75e-92549d32f408', embedding=None, metadata={'file_path': '/home/ec2-user/workspace/hfagnt-2-2-the-llamaindex-framework/data/persona_1.txt', 'file_name': 'persona_1.txt', 'file_type': 'text/plain', 'file_size': 266, 'creation_date': '2025-06-12', 'last_modified_date': '2025-06-12'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='655dd596-5f8b-4dea-acf2-da2298f44a0d', node_type='4', metadata={'file_path': '/home/ec2-user/workspace/hfagnt-2-2-the-llamaindex-framework/data/persona_1.txt', 'file_name': 'persona_1.txt', 'file_type': 'text/plain', 'file_size': 266, 'creation_date': '2025-06-12', 'last_modified_date': '2025-06-12'}, hash='933f06b703857eb8bc9e508970156db877211d3969e3695bf4865eb25c02624d')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='An anthropologist or a cultural expert interested in the intricacies of Cypriot culture, history, and society, particularly someone who has spent considerable time researching and living in Cyprus to gain a deep understanding of its people, customs, and way of life.', mimetype='text/plain', start_char_idx=0, end_char_idx=266, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.508559271837558), NodeWithScore(node=TextNode(id_='27a89989-f379-4ca7-afb8-86407307e362', embedding=None, metadata={'file_path': '/home/ec2-user/workspace/hfagnt-2-2-the-llamaindex-framework/data/persona_1.txt', 'file_name': 'persona_1.txt', 'file_type': 'text/plain', 'file_size': 266, 'creation_date': '2025-06-12', 'last_modified_date': '2025-06-12'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d6c1375a-9b54-4efc-908c-0be61abc4f45', node_type='4', metadata={'file_path': '/home/ec2-user/workspace/hfagnt-2-2-the-llamaindex-framework/data/persona_1.txt', 'file_name': 'persona_1.txt', 'file_type': 'text/plain', 'file_size': 266, 'creation_date': '2025-06-12', 'last_modified_date': '2025-06-12'}, hash='933f06b703857eb8bc9e508970156db877211d3969e3695bf4865eb25c02624d')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='An anthropologist or a cultural expert interested in the intricacies of Cypriot culture, history, and society, particularly someone who has spent considerable time researching and living in Cyprus to gain a deep understanding of its people, customs, and way of life.', mimetype='text/plain', start_char_idx=0, end_char_idx=266, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.508559271837558)], metadata={'8cbfc317-72ff-4cae-a75e-92549d32f408': {'file_path': '/home/ec2-user/workspace/hfagnt-2-2-the-llamaindex-framework/data/persona_1.txt', 'file_name': 'persona_1.txt', 'file_type': 'text/plain', 'file_size': 266, 'creation_date': '2025-06-12', 'last_modified_date': '2025-06-12'}, '27a89989-f379-4ca7-afb8-86407307e362': {'file_path': '/home/ec2-user/workspace/hfagnt-2-2-the-llamaindex-framework/data/persona_1.txt', 'file_name': 'persona_1.txt', 'file_type': 'text/plain', 'file_size': 266, 'creation_date': '2025-06-12', 'last_modified_date': '2025-06-12'}})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = await query_engine.aquery(\n",
    "    \"Respond using a persona that describes author and travel experiences?\"\n",
    ")\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
