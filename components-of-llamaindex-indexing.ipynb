{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "708b5568",
   "metadata": {},
   "source": [
    "## Create a `QueryEngine` for retrieval augmented generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd5f3b6",
   "metadata": {},
   "source": [
    "### Set up the environemnt first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "686fb7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98b2b11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_config import MyConfig\n",
    "my_config = MyConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9100bc",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "### Setting up the persona database\n",
    "We will be using personas from the dvilasuero/finepersonas-v0.1-tiny dataset. This dataset contains 5K personas that will be attending the party!\n",
    "Let's load the dataset and store it as files in the data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc799279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# from pathlib import Path\n",
    "\n",
    "# dataset = load_dataset(path=\"dvilasuero/finepersonas-v0.1-tiny\", split=\"train\")\n",
    "\n",
    "# Path(\"data\").mkdir(parents=True, exist_ok=True)\n",
    "# for i, persona in enumerate(dataset):\n",
    "#     with open(Path(\"data\") / f\"persona_{i}.txt\", \"w\") as f:\n",
    "#         f.write(persona[\"persona\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e818312",
   "metadata": {},
   "source": [
    "### Loading and embedding persona documents\n",
    "We will use the `SimpleDirectoryReader` to load the persona descriptions from the `data` directory. This will return a list of `Document` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddf64071",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/potter/Documents/MLArena/MLOps-Platform-Project/Hands-On/hfagnt-2-2-the-llamaindex-framework/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "reader = SimpleDirectoryReader(input_dir=\"data\")\n",
    "documents = reader.load_data()\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c87b79",
   "metadata": {},
   "source": [
    "Now we have a list of `Document` objects, we can use the `IngestionPipeline` to create nodes from the documents and prepare them for the `QueryEngine`.\n",
    "\n",
    "We will use the `SentenceSplitter` to split the documents into smaller chunks and the `HuggingFaceEmbedding` (via `vLLM` possibly) to embed the chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc65c85",
   "metadata": {},
   "source": [
    "#### Create a Custom Embedding Class\n",
    "We are subclassing `BaseEmbedding` from `llama_index` and override the `aget_text_embedding` method to hit my `RunPod` endpoint. Super Cool stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d673b6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementation of this class is in ./my_utils.py\n",
    "## The class is moved to utils, because we need to use it in the query notbook also\n",
    "from my_utils import RunPodEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e12ea1",
   "metadata": {},
   "source": [
    "### Storing and indexing documents\n",
    "Since we are using an ingestion pipeline, we can directly attach a vector store to the pipeline to populate it. In this case, we will use `Chroma` to store our documents. Let's run the pipeline again with the vector store attached. The `IngestionPipeline` caches the operations so this should be fast!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab5352b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 1 => 0:200\n",
      "Processed batch 2 => 200:400\n",
      "Processed batch 3 => 400:600\n",
      "Processed batch 4 => 600:800\n",
      "Processed batch 5 => 800:1000\n",
      "Processed batch 6 => 1000:1200\n",
      "Processed batch 7 => 1200:1400\n",
      "Processed batch 8 => 1400:1600\n",
      "Processed batch 9 => 1600:1800\n",
      "Processed batch 10 => 1800:2000\n",
      "Processed batch 11 => 2000:2200\n",
      "Processed batch 12 => 2200:2400\n",
      "Processed batch 13 => 2400:2600\n",
      "Processed batch 14 => 2600:2800\n",
      "Processed batch 15 => 2800:3000\n",
      "Processed batch 16 => 3000:3200\n",
      "Processed batch 17 => 3200:3400\n",
      "Processed batch 18 => 3400:3600\n",
      "Processed batch 19 => 3600:3800\n",
      "Processed batch 20 => 3800:4000\n",
      "Processed batch 21 => 4000:4200\n",
      "Processed batch 22 => 4200:4400\n",
      "Processed batch 23 => 4400:4600\n",
      "Processed batch 24 => 4600:4800\n",
      "Processed batch 25 => 4800:5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import chromadb\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "db = chromadb.PersistentClient(path=\"./alfred_chroma_db\")\n",
    "chroma_collection = db.get_or_create_collection(name=\"alfred\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "\n",
    "# Instantiate custom class with your RunPod URL\n",
    "runpod_url = f\"https://{my_config.VLLM_EMBEDDING_MODEL_INFERENCE_NODE_IP}-8000.proxy.runpod.net/v1/embeddings\"\n",
    "embedding_model = RunPodEmbedding(endpoint_url=runpod_url)\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(),\n",
    "        embedding_model\n",
    "    ],\n",
    "    vector_store=vector_store,\n",
    ")\n",
    "\n",
    "nodes = list()\n",
    "for batch in range(0, len(documents), 200):\n",
    "    nodes.extend(await pipeline.arun(documents=documents[batch:batch+200]))\n",
    "    print(f\"Processed batch {batch // 200 + 1} => {batch}:{batch + 200}\")\n",
    "    import time; time.sleep(2) # <- Defensive guard against too many parallel requests\n",
    "len(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a074900",
   "metadata": {},
   "source": [
    "We can create a `VectorStoreIndex` from the vector store and use it to query the documents by passing the vector store and embedding model to the `from_vector_store()` method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
